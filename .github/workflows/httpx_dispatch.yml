name: Distributed httprobe

on:
  repository_dispatch:
    types: [trigger_http_multiple]

  workflow_dispatch:  

permissions:
  contents: write   # For creating matrix.json, artifacts, and potentially committing aggregated results
  actions: write    # For triggering workflows in Account 2's repository

env:
  LINES_PER_CHUNK: 30
  DISTRIBUTION_THRESHOLD: 21    # <-- LINKED VARIABLE
  PRIMARY_PERCENTAGE: 45        # <-- LINKED VARIABLE
  SECONDARY_PERCENTAGE: 40      # <-- LINKED VARIABLE

  # --- Account 2 Details ---
  ACCOUNT2_REPO_OWNER: ${{ secrets.ACCOUNT2_REPO_OWNER || 'pushrockzz' }}
  ACCOUNT2_REPO_NAME: ${{ secrets.ACCOUNT2_REPO_NAME || 'httpx-rec' }}

  # --- Account 3 Details ---
  ACCOUNT3_REPO_OWNER: ${{ secrets.ACCOUNT3_REPO_OWNER || 'Sari2705' }}      
  ACCOUNT3_REPO_NAME: ${{ secrets.ACCOUNT3_REPO_NAME || 'httpx-rec3' }}       


jobs:
  prepare_all_chunks_and_package:
    name: Prepare All Chunks & Package
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ secrets.GHCR_USER }}
        password: ${{ secrets.GHCR_TOKEN }}
    outputs:
      # Output 1: The full JSON matrix string of ALL generated chunks
      all_chunks_matrix_json: ${{ steps.build_full_matrix.outputs.full_matrix }}
      # Output 2: The total number of chunks generated
      total_chunks_count: ${{ steps.build_full_matrix.outputs.total_chunks }}
      # Output 3: The name of the artifact containing all chunks and resolvers
      chunk_package_artifact_name: "all-chunks-package-${{ github.run_id }}"
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: DEBUG - Verify Tools and Initial File System State
        shell: bash
        run: |
          echo "DEBUG: Current directory: $(pwd)"
          echo "DEBUG: Listing all files in repository root and subdirectories:"
          find . -print
          echo "-----------------------------------------------------"
          echo "DEBUG: Specifically looking for subdomains.txt files:"
          find . -type f -name 'subdomains.txt' -print || echo "DEBUG: 'find' for subdomains.txt failed or found nothing."
          echo "-----------------------------------------------------"
          echo "DEBUG: Checking for 'jq' command..."
          if ! command -v jq &> /dev/null; then
            echo "CRITICAL_ERROR: 'jq' command not found. This is essential. Aborting."
            exit 0
          else
            echo "DEBUG: jq version: $(jq --version)"
          fi
          echo "-----------------------------------------------------"
          echo "DEBUG: Checking for 'split' command..."
          if ! command -v split &> /dev/null; then
            echo "CRITICAL_ERROR: 'split' command not found. This is essential. Aborting."
            exit 0
          else
            echo "DEBUG: split version information (if available):"
            split --version || echo "DEBUG: Could not get split version."
          fi
          echo "-----------------------------------------------------"
      
      - name: Download Subdomain Artifacts from Source Repo
        uses: actions/download-artifact@v4
        with:

          github-token: ${{ secrets.PAT_TOKEN }}
          # This tells the action to get artifacts from the specific workflow run that triggered this one.
          run-id: ${{ github.event.client_payload.run_id  }}
          repository:  ${{ github.event.client_payload.results_repo }}
          pattern: puredns-final-sorted-*
          path: downloaded-artifacts

      - name: DEBUG - Show Downloaded Artifact Structure
        if: always() # This ensures the step runs even if the download step had warnings
        shell: bash
        run: |
          echo "======================================================================"
          echo "               DEBUGGING DOWNLOADED ARTIFACTS                         "
          echo "======================================================================"
          
          ARTIFACTS_DIR="downloaded-artifacts"

          echo "INFO: Checking if the artifacts directory '$ARTIFACTS_DIR' exists..."
          if [ ! -d "$ARTIFACTS_DIR" ]; then
            echo "ERROR: The directory '$ARTIFACTS_DIR' was NOT found."
            echo "This likely means the artifact download failed or the artifact was empty."
            exit 0 # Exit with an error to make the problem obvious
          fi

          echo ""
          echo "--- Option A: Recursive Listing (Comprehensive & Always Available) ---"
          echo "This shows all downloaded files and their directories."
          ls -R "$ARTIFACTS_DIR"

          echo ""
          echo "--- Option B: Tree View (More Visual, if 'tree' is installed) ---"
          # 'tree' gives a nicer visual output but may not be installed.
          # We check for it first to avoid errors.
          if command -v tree &> /dev/null; then
            echo "The 'tree' command is available. Displaying visual structure:"
            tree "$ARTIFACTS_DIR"
          else
            echo "The 'tree' command is not available in this container. Skipping tree view."
          fi
          
          echo ""
          echo "--- Verifying Target Files ---"
          echo "This shows the exact 'subdomains_ports.txt' files that will be processed by the next step:"
          find "$ARTIFACTS_DIR" -type f -name "subdomains_ports.txt" -print
          
          echo "======================================================================"

          
      - name: Build Full Matrix & Create Chunks
        id: build_full_matrix
        shell: bash
        run: |
          # --- Start of Script ---
          ARTIFACTS_DIR="downloaded-artifacts"
          COMBINED_TARGET_FILE="subdomains.txt" 

          echo "INFO: Preparing input from downloaded artifacts..."
          if [ ! -d "$ARTIFACTS_DIR" ] || [ -z "$(ls -A $ARTIFACTS_DIR)" ]; then
             echo "WARNING: Artifacts directory '$ARTIFACTS_DIR' is missing or empty..."
          else
             echo "INFO: Finding all 'subdomains_ports.txt' files and combining them into '$COMBINED_TARGET_FILE'..."
             find "$ARTIFACTS_DIR" -type f -name "subdomains_ports.txt" -exec cat {} + > "$COMBINED_TARGET_FILE"

          fi
           echo "--- Pre-processing complete. Starting original script logic. ---"

          JSON_MATRIX='[]' # Initialize as an empty JSON array string
          echo "INFO: Initializing JSON_MATRIX as: '$JSON_MATRIX'"

          echo "INFO: Locating 'subdomains.txt' files..."
          find . -type f -name "subdomains.txt" -print0 > found_files.tmp

          declare -a files=() # Explicitly declare as an array
          while IFS= read -r -d $'\0' file_path_from_find; do
            standardized_file_path=$(echo "$file_path_from_find" | sed 's|^\./||')
            files+=("$standardized_file_path")
          done < found_files.tmp
          rm found_files.tmp # Clean up temporary file

          if [ "${#files[@]}" -eq 0 ]; then
            echo "WARNING: No 'subdomains.txt' files found in the repository."
            echo "INFO: JSON_MATRIX will remain '[]'. No chunks will be generated."
          else
            echo "INFO: Found ${#files[@]} 'subdomains.txt' file(s):"
            printf "  => '%s'\n" "${files[@]}" # Print each found file for clarity

            for file_path in "${files[@]}"; do
              echo "-----------------------------------------------------"
              echo "INFO: Processing file: '$file_path'"

              domain_dir=$(dirname "$file_path")
              if [ "$domain_dir" == "." ]; then
                domain=$(basename "$file_path" .txt)
              else
                domain=$(basename "$domain_dir")
              fi
              echo "INFO: Deduced domain: '$domain'"

              if [ ! -f "$file_path" ]; then
                echo "ERROR: File '$file_path' reported by find, but not found now. Skipping."
                continue
              fi
              if [ ! -s "$file_path" ]; then
                echo "WARNING: File '$file_path' is empty. Skipping splitting for this file."
                continue
              fi

              echo "INFO: Creating chunk directory 'chunks/$domain' if it doesn't exist."
              mkdir -p "chunks/$domain"

              output_chunk_prefix="chunks/$domain/chunk_"
              echo "INFO: Splitting '$file_path' into chunks in 'chunks/$domain/' with prefix '$output_chunk_prefix' (max $LINES_PER_CHUNK lines/chunk)"
              
              split -l "$LINES_PER_CHUNK" -a 3 --numeric-suffixes=1 "$file_path" "$output_chunk_prefix"
              split_exit_code=$?
              if [ $split_exit_code -ne 0 ]; then
                echo "ERROR: 'split' command failed with exit code $split_exit_code for file '$file_path'. Skipping chunk generation for this file."
                continue
              fi
              
              CHUNK_COUNT_FOR_THIS_FILE=0
              while IFS= read -r chunk_file_path; do
                if [ -z "$chunk_file_path" ]; then continue; fi

                echo "DEBUG: Processing chunk_file_path: '$chunk_file_path' for domain '$domain'"
                echo "DEBUG: Current JSON_MATRIX before jq (first 100 chars): $(echo "$JSON_MATRIX" | head -c 100)"
                echo "DEBUG: Validating current JSON_MATRIX with jq -e:"
                if echo "$JSON_MATRIX" | jq -e . > /dev/null 2>&1; then
                  echo "DEBUG: Current JSON_MATRIX is VALID."
                else
                  echo "CRITICAL_ERROR_DETECTED: Current JSON_MATRIX is INVALID before processing '$chunk_file_path'."
                  echo "CRITICAL_ERROR_DETECTED: JSON_MATRIX content: $JSON_MATRIX"
                  if [ "$JSON_MATRIX" != "[]" ] && [ "$CHUNK_COUNT_FOR_THIS_FILE" -gt 0 ]; then
                      echo "ABORTING chunk addition for this file due to previously corrupted JSON_MATRIX."
                      break 
                  else
                      echo "WARNING: JSON_MATRIX was invalid but appeared to be at an initial state. Attempting to reset to []."
                      JSON_MATRIX="[]"
                  fi
                fi
                
                if ! command -v jq &> /dev/null; then
                  echo "CRITICAL_ERROR: 'jq' command not found during chunk processing. Aborting this file's chunk addition."
                  break 
                fi
                
                echo "DEBUG: jq command to be run: printf '%s' \"\$JSON_MATRIX\" | jq -c --arg d \"$domain\" --arg c \"$chunk_file_path\" '. + [{domain:\$d,chunk:\$c}]'"
                TEMP_JSON_MATRIX=$(printf '%s' "$JSON_MATRIX" | jq -c --arg d "$domain" --arg c "$chunk_file_path" '. + [{domain:$d,chunk:$c}]')
                jq_exit_code=$?

                if [ $jq_exit_code -ne 0 ]; then
                  echo "ERROR: jq command failed (exit code $jq_exit_code) when trying to add chunk '$chunk_file_path' for domain '$domain'."
                  echo "ERROR: Input JSON_MATRIX was (first 100 chars): $(echo "$JSON_MATRIX" | head -c 100)"
                  echo "ERROR: Arguments to jq were: domain='$domain', chunk_file_path='$chunk_file_path'"
                  continue
                elif [ -z "$TEMP_JSON_MATRIX" ]; then
                  echo "ERROR: jq command produced empty output for chunk '$chunk_file_path'. This should not happen with '. + [...]'."
                  echo "ERROR: Input JSON_MATRIX that led to empty output (first 200 chars): $(echo "$JSON_MATRIX" | head -c 200)"
                  echo "ERROR: Full input JSON_MATRIX that led to empty output: $JSON_MATRIX"
                  echo "ERROR: Arguments to jq were: domain='$domain', chunk_file_path='$chunk_file_path'"
                  echo "DEBUG: Re-validating the input JSON_MATRIX that caused empty output:"
                  if echo "$JSON_MATRIX" | jq -e . > /dev/null 2>&1; then
                      echo "DEBUG: Input JSON_MATRIX was still considered VALID by jq -e just before the empty output. This is very strange."
                  else
                      echo "DEBUG: Input JSON_MATRIX was considered INVALID by jq -e. This is the likely cause of empty output."
                  fi
                  if [ "$JSON_MATRIX" == "" ]; then
                      echo "DEBUG: JSON_MATRIX was an empty string. Trying to initialize with the current chunk."
                      TEMP_JSON_MATRIX=$(jq -cn --arg d "$domain" --arg c "$chunk_file_path" '[{domain:$d,chunk:$c}]')
                      if [ -n "$TEMP_JSON_MATRIX" ]; then
                          echo "DEBUG: Successfully initialized TEMP_JSON_MATRIX from empty string case."
                      else
                          echo "ERROR: Still failed to initialize TEMP_JSON_MATRIX even from empty string case."
                          continue
                      fi
                  else
                      continue 
                  fi
                fi

                JSON_MATRIX="$TEMP_JSON_MATRIX"
                CHUNK_COUNT_FOR_THIS_FILE=$((CHUNK_COUNT_FOR_THIS_FILE + 1))
                echo "DEBUG: Successfully added/updated for chunk '$chunk_file_path'. JSON_MATRIX (first 100 chars): $(echo "$JSON_MATRIX" | head -c 100)"
              done < <(find "chunks/$domain/" -name 'chunk_*' -type f -print)

              if [ "$CHUNK_COUNT_FOR_THIS_FILE" -eq 0 ]; then
                echo "WARNING: No chunk files were processed/added to matrix for '$file_path' in domain '$domain'. 'split' might have created no files, or 'jq' failed for all."
              else
                echo "INFO: Processed and added $CHUNK_COUNT_FOR_THIS_FILE chunk(s) to matrix for domain '$domain' from '$file_path'."
              fi
            done
          fi

          echo "-----------------------------------------------------"
          if ! echo "$JSON_MATRIX" | jq -e . > /dev/null 2>&1; then
            echo "ERROR: Final JSON_MATRIX is not valid JSON. Content (first 200 chars): $(echo "$JSON_MATRIX" | head -c 200)"
            echo "WARNING: Setting JSON_MATRIX to '[]' and total_chunks to 0 due to invalid JSON."
            JSON_MATRIX="[]"
            TOTAL_CHUNKS=0
          else
            TOTAL_CHUNKS=$(echo "$JSON_MATRIX" | jq 'length')
          fi

          echo "FINAL_BUILD_INFO: Final JSON_MATRIX content (first 200 chars): $(echo "$JSON_MATRIX" | head -c 200)"
          echo "FINAL_BUILD_INFO: Total chunks in matrix: $TOTAL_CHUNKS"

          echo "INFO: Writing the final matrix to 'full_matrix.json' file..."
          echo "$JSON_MATRIX" > full_matrix.json

          if [ -f full_matrix.json ]; then
            echo "VERIFY: 'full_matrix.json' created. Size: $(wc -c < full_matrix.json) bytes."
            echo "VERIFY: Content preview of 'full_matrix.json' (first 200 chars): $(head -c 200 full_matrix.json)"
            if jq -e . full_matrix.json > /dev/null 2>&1; then
              echo "VERIFY: 'full_matrix.json' contains valid JSON."
            else
              echo "ERROR: 'full_matrix.json' does NOT contain valid JSON! This will cause issues for 'fromJson' later."
            fi
          else
            echo "CRITICAL_ERROR: 'full_matrix.json' was NOT created after attempting to write JSON_MATRIX."
            echo "INFO: Forcing 'full_matrix.json' to be '[]' as a fallback to prevent tar error."
            echo "[]" > full_matrix.json
          fi

          echo "full_matrix=$JSON_MATRIX" >> $GITHUB_OUTPUT
          echo "total_chunks=$TOTAL_CHUNKS" >> $GITHUB_OUTPUT                 

      - name: Package All Chunks and Resolvers      
        id: package_chunks # This ID is referenced by outputs and later steps
        if: steps.build_full_matrix.outputs.total_chunks > 0
        shell: bash
        run: |
          # Define the base name for the artifact (without .tar.gz)
          BASE_ARTIFACT_NAME="all-chunks-package-${{ github.run_id }}"
          # Define the full tar filename
          PACKAGE_TAR_FILENAME="${BASE_ARTIFACT_NAME}.tar.gz"

          echo "INFO: Preparing package '$PACKAGE_TAR_FILENAME'..."
          # Ensure resolver files and full_matrix.json exist, create fallbacks if not
          if [ ! -f resolvers.txt ]; then echo "WARNING: resolvers.txt not found, creating default." >&2; echo '1.1.1.1' > resolvers.txt; fi
          if [ ! -f resolvers-trusted.txt ]; then echo "WARNING: resolvers-trusted.txt not found, creating empty." >&2; touch resolvers-trusted.txt; fi
          if [ ! -f full_matrix.json ]; then echo "WARNING: full_matrix.json not found, creating empty array JSON file." >&2; echo "[]" > full_matrix.json; fi
          
          # Create the tarball
          echo "INFO: Creating tarball: $PACKAGE_TAR_FILENAME"
          tar -czvf "$PACKAGE_TAR_FILENAME" chunks resolvers.txt resolvers-trusted.txt full_matrix.json
          
          if [ $? -eq 0 ]; then
            echo "INFO: '$PACKAGE_TAR_FILENAME' created successfully."
          else
            echo "ERROR: Failed to create tarball '$PACKAGE_TAR_FILENAME'."
            # Exit or handle error appropriately if tar fails
            exit 0 
          fi
          
          # Output the full .tar.gz filename for the 'path' in upload-artifact
          
          echo "package_tar_filename=$PACKAGE_TAR_FILENAME" >> $GITHUB_OUTPUT
          
          # Output the base artifact name (without .tar.gz) for the 'name' in upload-artifact and for job output
          
          echo "base_artifact_name=$BASE_ARTIFACT_NAME" >> $GITHUB_OUTPUT

      - name: Upload Full Chunks Package
        if: steps.build_full_matrix.outputs.total_chunks > 0 && steps.package_chunks.outcome == 'success'
        uses: actions/upload-artifact@v4
        with:
          # Use the base artifact name output from the 'package_chunks' step
          name: ${{ steps.package_chunks.outputs.base_artifact_name }}
          # Use the full tar filename output from the 'package_chunks' step
          path: ${{ steps.package_chunks.outputs.package_tar_filename }}
          retention-days: 1

  distribute_and_trigger_secondary:
    name: Distribute Work & Trigger Secondary
    needs: prepare_all_chunks_and_package
    if: needs.prepare_all_chunks_and_package.outputs.total_chunks_count > 0 # Only run if there are chunks
    runs-on: ubuntu-latest
    outputs:
      primary_matrix_json: ${{ steps.calculate_distribution.outputs.primary_matrix }}
      secondary_matrix_json: ${{ steps.calculate_distribution.outputs.secondary_matrix }}
      tertiary_matrix_json: ${{ steps.calculate_distribution.outputs.tertiary_matrix }}
      secondary_processing_triggered: ${{ steps.trigger_secondary.outcome == 'success' && steps.calculate_distribution.outputs.secondary_chunks_exist == 'true' }}
            
      tertiary_processing_triggered: ${{ steps.trigger_tertiary.outcome == 'success' && steps.calculate_distribution.outputs.tertiary_chunks_exist == 'true' }}
      
    steps:
      
      - name: Calculate Chunk Distribution for Accounts
        id: calculate_distribution
        shell: bash
        run: |
          ALL_CHUNKS_JSON='${{ needs.prepare_all_chunks_and_package.outputs.all_chunks_matrix_json }}'
          TOTAL_CHUNKS=${{ needs.prepare_all_chunks_and_package.outputs.total_chunks_count }}

           # This is where the script reads the control variable
          THRESHOLD=${{ env.DISTRIBUTION_THRESHOLD }}
          
          echo "Full matrix (first 200 chars): $(echo "$ALL_CHUNKS_JSON" | head -c 200)"
          

          echo "Total chunks generated: $TOTAL_CHUNKS"
          echo "Distribution threshold: $THRESHOLD"

          if [ "$TOTAL_CHUNKS" -lt "$THRESHOLD" ]; then
            # STRATEGY 1: Below threshold, primary gets everything
            echo "Total chunks are below threshold. Assigning all work to Primary Account."
            PRIMARY_CHUNKS_JSON="$ALL_CHUNKS_JSON"
            SECONDARY_CHUNKS_JSON="[]"
            TERTIARY_CHUNKS_JSON="[]"
          else

            # STRATEGY 2: At or above threshold, percentage split is performed
            
            echo "Total chunks are at or above threshold. Applying percentage split."
            
            # Reading the percentage variables
            
            PRIMARY_PERCENT=${{ env.PRIMARY_PERCENTAGE }}
            SECONDARY_PERCENT=${{ env.SECONDARY_PERCENTAGE }}

            # Calculating the number of chunks for each account using 'bc' for math
            
            PRIMARY_CHUNK_COUNT=$(echo "($TOTAL_CHUNKS * $PRIMARY_PERCENT) / 100" | bc)
            SECONDARY_CHUNK_COUNT=$(echo "($TOTAL_CHUNKS * $SECONDARY_PERCENT) / 100" | bc)
            OFFSET_FOR_TERTIARY=$((PRIMARY_CHUNK_COUNT + SECONDARY_CHUNK_COUNT))

            # Slicing the JSON array with jq to create the new, smaller matrices
            PRIMARY_CHUNKS_JSON=$(echo "$ALL_CHUNKS_JSON" | jq -c --argjson limit "$PRIMARY_CHUNK_COUNT" '.[0:$limit]')
            SECONDARY_CHUNKS_JSON=$(echo "$ALL_CHUNKS_JSON" | jq -c --argjson offset "$PRIMARY_CHUNK_COUNT" --argjson limit "$SECONDARY_CHUNK_COUNT" '.[$offset : $offset+$limit]')
            TERTIARY_CHUNKS_JSON=$(echo "$ALL_CHUNKS_JSON" | jq -c --argjson offset "$OFFSET_FOR_TERTIARY" '.[$offset:]')
          fi  

          echo "primary_matrix=$PRIMARY_CHUNKS_JSON" >> $GITHUB_OUTPUT
          echo "secondary_matrix=$SECONDARY_CHUNKS_JSON" >> $GITHUB_OUTPUT
          echo "tertiary_matrix=$TERTIARY_CHUNKS_JSON" >> $GITHUB_OUTPUT
          
          # Setting boolean flags for easier conditional checks later
          if [ "$(echo "$SECONDARY_CHUNKS_JSON" | jq 'length')" -eq 0 ]; then echo "secondary_chunks_exist=false" >> $GITHUB_OUTPUT; else echo "secondary_chunks_exist=true" >> $GITHUB_OUTPUT; fi
          if [ "$(echo "$TERTIARY_CHUNKS_JSON" | jq 'length')" -eq 0 ]; then echo "tertiary_chunks_exist=false" >> $GITHUB_OUTPUT; else echo "tertiary_chunks_exist=true" >> $GITHUB_OUTPUT; fi

      - name: Add Delay Between API Triggers For Secondary Account
        if: steps.calculate_distribution.outputs.secondary_chunks_exist == 'true'
        run: |
          echo "Pausing for 15 seconds to avoid API rate-limiting before triggering the Secondary account..."
          sleep 15
      
      - name: Prepare Trigger Payload For Secondry
        id: prepare_payload
        if: steps.calculate_distribution.outputs.secondary_chunks_exist == 'true'
        shell: bash
        run: |
          # Ensure the secondary_matrix output from the previous step is treated as a string
          # that already represents a JSON array. jq's --argjson will parse it.
          
          SECONDARY_MATRIX_AS_STRING='${{ steps.calculate_distribution.outputs.secondary_matrix }}'

          # Validate that SECONDARY_MATRIX_AS_STRING is actually valid JSON before --argjson
          if ! echo "${SECONDARY_MATRIX_AS_STRING}" | jq -e . > /dev/null 2>&1; then
            echo "::error title=Invalid Secondary Matrix::The secondary_matrix output ('${SECONDARY_MATRIX_AS_STRING}') is not valid JSON. Defaulting to empty array."
            SECONDARY_MATRIX_AS_STRING="[]" # Fallback to an empty JSON array string
          fi

          # Construct the entire payload as a valid JSON string using jq
          # --argjson tells jq to parse the value of matrix_json as JSON, not treat it as a literal string
          JSON_PAYLOAD=$(jq -cn \
            --arg run_id "${{ github.run_id }}" \
            '{
              "primary_run_id": $run_id
            }')
             echo "Constructed JSON Payload: $JSON_PAYLOAD"
             echo "json_string=$JSON_PAYLOAD" >> "$GITHUB_OUTPUT"
           
          JSON_PAYLOAD=$(jq -cn \
            --arg server_url "${{ github.server_url }}" \
            --arg repo_owner "${{ github.repository_owner }}" \
            --arg repo_name "${{ github.event.repository.name }}" \
            --arg run_id "${{ github.run_id }}" \
            --arg commit_pattern "feat(http-multiple): Automated http-multiple results from Correlation ID:" \
            --arg artifact_name "${{ needs.prepare_all_chunks_and_package.outputs.chunk_package_artifact_name }}" \
            --arg matrix_as_a_string "${SECONDARY_MATRIX_AS_STRING}" \
            '{
              "primary_github_server_url": $server_url,
              "primary_repo_owner": $repo_owner,
              "primary_repo_name": $repo_name,
              "primary_run_id": $run_id,
              "commit_pattern": $commit_pattern,
              "chunk_package_artifact_name": $artifact_name,
              "secondary_matrix_json": $matrix_as_a_string 
            }')
          
          echo "Constructed JSON Payload: $JSON_PAYLOAD"
          # Set the constructed JSON string as an output of this step
          echo "json_string=$JSON_PAYLOAD" >> $GITHUB_OUTPUT
      
      - name: DEBUG - Show Exact Inputs String
        id: debug_show_inputs
        run: |
          INPUTS_STRING="${{ steps.prepare_payload.outputs.json_string }}"
          echo "DEBUG: Exact string value being passed to 'inputs':"
          echo "$INPUTS_STRING"
          echo "DEBUG: Length of inputs string: ${#INPUTS_STRING}"

      - name: Trigger Secondary Account Workflow
        id: trigger_secondary 
        if: steps.calculate_distribution.outputs.secondary_chunks_exist == 'true'
        uses: benc-uk/workflow-dispatch@v1
        with:
          workflow: httpx_receive.yml
          repo: ${{ env.ACCOUNT2_REPO_OWNER }}/${{ env.ACCOUNT2_REPO_NAME }}
          token: ${{ secrets.PAT_FOR_SECONDARY_ACCOUNT_REPO }}
          inputs: ${{ steps.prepare_payload.outputs.json_string }}
          ref: main # Or your default branch name in Account 2
      
      - name: Add Delay Between API Triggers for Tertiary Account
        if: steps.calculate_distribution.outputs.tertiary_chunks_exist == 'true'
        run: |
          echo "Pausing for 15 seconds to avoid API rate-limiting before triggering the Tertiary account..."
          sleep 60

      - name: Prepare Trigger Payload For Tertiary
        id: prepare_payload_tertiary
        if: steps.calculate_distribution.outputs.tertiary_chunks_exist == 'true'
        shell: bash
        run: |
          MATRIX_AS_STRING='${{ steps.calculate_distribution.outputs.tertiary_matrix }}'
          JSON_PAYLOAD=$(jq -cn \
            --arg server_url "${{ github.server_url }}" \
            --arg repo_owner "${{ github.repository_owner }}" \
            --arg repo_name "${{ github.event.repository.name }}" \
            --arg run_id "${{ github.run_id }}" \
            --arg commit_pattern "feat(http-multiple): Automated http-multiple results from Correlation ID:" \
            --arg artifact_name "${{ needs.prepare_all_chunks_and_package.outputs.chunk_package_artifact_name }}" \
            --arg matrix_as_a_string "${MATRIX_AS_STRING}" \
            '{
              "primary_github_server_url": $server_url,
              "primary_repo_owner": $repo_owner,
              "primary_repo_name": $repo_name,
              "primary_run_id": $run_id,
              "commit_pattern": $commit_pattern,
              "chunk_package_artifact_name": $artifact_name,
              "tertiary_matrix_json": $matrix_as_a_string 
            }')
          echo "json_string=$JSON_PAYLOAD" >> $GITHUB_OUTPUT
     
      - name: DEBUG - Show Exact Inputs String
        id: debug_show_inputs_tertiary
        run: |
          INPUTS_STRING="${{ steps.prepare_payload_tertiary.outputs.json_string }}"
          echo "DEBUG: Exact string value being passed to 'inputs':"
          echo "$INPUTS_STRING"
          echo "DEBUG: Length of inputs string: ${#INPUTS_STRING}"
          
      - name: Trigger Tertiary Account Workflow
        id: trigger_tertiary
        if: steps.calculate_distribution.outputs.tertiary_chunks_exist == 'true'
        uses: benc-uk/workflow-dispatch@v1
        with:
          workflow: httpx_receive.yml # The workflow file name in the tertiary repo
          repo: ${{ env.ACCOUNT3_REPO_OWNER }}/${{ env.ACCOUNT3_REPO_NAME }}
          token: ${{ secrets.PAT_FOR_TERTIARY_ACCOUNT_REPO }} # You will need to add this secret
          inputs: ${{ steps.prepare_payload_tertiary.outputs.json_string }}
          ref: main

  resolve_primary_account_chunks:
    name: Resolve Primary Account Chunks (httpx + dsieve)
    needs: [prepare_all_chunks_and_package, distribute_and_trigger_secondary] # Depends on distribution logic
    if: needs.prepare_all_chunks_and_package.outputs.total_chunks_count > 0 && needs.distribute_and_trigger_secondary.outputs.primary_matrix_json != '[]'
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GHCR_TOKEN }}    
    strategy:
      fail-fast: false
      max-parallel: 20 
      matrix:
        pair: ${{ fromJson(needs.distribute_and_trigger_secondary.outputs.primary_matrix_json) }}
    steps:
      - name: Checkout repository (for results structure)
        uses: actions/checkout@v3

      - name: Download Full Chunks Package for Primary
        uses: actions/download-artifact@v4
        with:
          name: ${{ needs.prepare_all_chunks_and_package.outputs.chunk_package_artifact_name }}
          # Downloads to current directory

      - name: Extract Chunks for Primary
        shell: bash
        run: |
          PACKAGE_FILENAME="${{ needs.prepare_all_chunks_and_package.outputs.chunk_package_artifact_name }}.tar.gz"
          echo "Extracting $PACKAGE_FILENAME..."
          tar -xzvf "$PACKAGE_FILENAME"
          if [ ! -d "chunks" ]; then
             echo "ERROR: 'chunks/' not found after extraction!"
             exit 0
          fi
          echo "Extraction complete. 'chunks/' should be present."
          ls -R chunks/

      - name: Set up Go & install httpx + dsieve
        uses: actions/setup-go@v4
        with:
          go-version: '1.23'
      - name: Install httpx and dsieve
        run: |
          if command -v dsieve &> /dev/null; then
            echo "dsieve is already installed"
          else
            echo "Installing dsieve..."
            go install github.com/trickest/dsieve@latest
          fi

      - name: Run httpx + pre-dsieve on subdomains + final filtering
        shell: bash
        run: |
          DOMAIN=${{ matrix.pair.domain }}
          CHUNK_FILE_PATH=${{ matrix.pair.chunk }}

          # --- CHANGE 1: Create a unique identifier for this job's files ---
          SAFE_CHUNK_ID=$(echo "${{ matrix.pair.chunk }}" | tr '/' '_')
          
          echo "Processing domain '$DOMAIN' with chunk '$CHUNK_FILE_PATH' (ID: $SAFE_CHUNK_ID)..."

          if [ ! -f "$CHUNK_FILE_PATH" ]; then
            echo "ERROR: Chunk file '$CHUNK_FILE_PATH' not found!"
            exit 0
          fi

          # --- CHANGE 2a: Write to a unique parent domains file ---
          PARENT_DOMAINS_FILE="parent_domains_${SAFE_CHUNK_ID}.txt"
          echo "-> Generating $PARENT_DOMAINS_FILE from raw subdomains..."
          dsieve -if "$CHUNK_FILE_PATH" -f 2 | sort -u > "$PARENT_DOMAINS_FILE"
          
          # --- CHANGE 2b: Write httpx output to a unique file ---
          HTTPX_OUT="httpx_output_${SAFE_CHUNK_ID}.txt"
          echo "-> Running httpx against '$CHUNK_FILE_PATH'..."
          httpx \
            -l "$CHUNK_FILE_PATH" -threads 80 \
            -sc -cl -vhost -websocket -fhr \
            -retries 2 -timeout 30 -delay 100ms \
            -random-agent -silent -no-color \
            -o "$HTTPX_OUT"
            
          if [ ! -s "$HTTPX_OUT" ]; then
            echo "No live URLs found in this chunk. Exiting early."
            mkdir -p results
            exit 0
          fi
          
          OUTPUT_ROOT="results"
          mkdir -p "$OUTPUT_ROOT"
          
          # Read from the unique parent domains file
          while read -r parent; do
            mkdir -p "$OUTPUT_ROOT/$parent"
            
            # --- CHANGE 3: Append (>>) to the final file instead of overwriting (>) ---
            grep -iE "(^|https?:\/\/|[^a-zA-Z0-9-])${parent}" "$HTTPX_OUT" --color=never >> "$OUTPUT_ROOT/$parent/httpx_result.txt" || true
              
          done < "$PARENT_DOMAINS_FILE"
          
          echo "-> Processing complete for chunk $SAFE_CHUNK_ID."
      
      - name: Compute SAFE_CHUNK (no slashes)
        run: |
          SAFE_CHUNK="${{ matrix.pair.chunk }}"
          SAFE_CHUNK="$(echo "$SAFE_CHUNK" | tr '/' '_')"
          echo "SAFE_CHUNK=$SAFE_CHUNK" >> $GITHUB_ENV         
      
      - name: Upload Primary Account httpx+dsieve Results
        uses: actions/upload-artifact@v4
        with:
          name: primary_httpx_dsieve_${{ matrix.pair.domain }}_${{ env.SAFE_CHUNK }}
          path: results/
          retention-days: 1

  merge_results:
    name: Merge All httpx Results
    runs-on: ubuntu-latest
    # This job needs to run only after all the parallel 'resolve' jobs are complete.
    needs: resolve_primary_account_chunks
    # 'if: always()' ensures this job runs even if some of the parallel jobs failed,
    # so you can collect results from the successful ones.
    if: always()
    steps:
      - name: Download all result artifacts into separate directories
        uses: actions/download-artifact@v4
        with:
          # This pattern still correctly matches all artifacts
          pattern: primary_httpx_dsieve_*
          # By NOT using 'merge-multiple', each artifact is downloaded into its own named sub-directory.
          # This prevents any files from overwriting each other and is the key to preventing data loss.
          path: temp-artifacts/

      - name: DEBUG - Show Downloaded Artifact Structure
        if: always()
        run: |
          echo "Listing downloaded artifacts structure in 'temp-artifacts':"
          # This will now show a directory for each downloaded artifact, preventing data loss.
          ls -R temp-artifacts/
          
      - name: Consolidate, Merge, and Deduplicate Results
        id: consolidate
        shell: bash
        run: |
          # Directory for the final, clean output
          FINAL_RESULTS_DIR="final-merged-results"
          mkdir -p "$FINAL_RESULTS_DIR"

          # Parent directory where download-artifact placed all the individual artifact folders
          SOURCE_ARTIFACTS_DIR="temp-artifacts"

          # Check if any artifacts were downloaded
          if [ ! -d "$SOURCE_ARTIFACTS_DIR" ] || [ -z "$(ls -A $SOURCE_ARTIFACTS_DIR)" ]; then
            echo "No artifacts were downloaded. Nothing to merge."
            echo "artifact_created=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          echo "-> Finding all result files across all downloaded artifacts..."

          # This 'find' command is the core of the aggregation.
          # It searches through all the downloaded artifact sub-directories for any file named 'httpx_result.txt'.
          find "$SOURCE_ARTIFACTS_DIR" -type f -name "httpx_result.txt" | while read -r filepath; do
            # --- HOW DATA IS SEPARATED BY DOMAIN (This is the key logic) ---
            # 1. The original 'filepath' will look something like this:
            #    'temp-artifacts/primary_httpx_dsieve_bmw.com_..._chunk_027/results/bmw.com/httpx_result.txt'
            #
            # 2. 'dirname "$filepath"' gets the full directory path:
            #    'temp-artifacts/primary_httpx_dsieve_bmw.com_..._chunk_027/results/bmw.com'
            #
            # 3. 'basename' on that result extracts just the last component, which is the parent domain name:
            #    'bmw.com'
            parent_domain=$(basename "$(dirname "$filepath")")

            # Create a directory for this domain in our final output if it doesn't exist
            mkdir -p "$FINAL_RESULTS_DIR/$parent_domain"

            # Append (>>) the content of the current result file to the master result file for that domain.
            # The '>>' operator is crucial as it adds to the file, rather than overwriting it ('_').
            # This ensures that results from chunk_001, chunk_002, etc., for 'bmw.com' all end up in the same final file.
            cat "$filepath" >> "$FINAL_RESULTS_DIR/$parent_domain/httpx_result.txt"
          done

          echo "-> Aggregation complete. Now deduplicating all merged files..."

          # After all files have been aggregated, this command finds every final results file
          # and runs 'sort -u' on it. This removes any duplicate lines that might have
          # occurred across different chunks, ensuring a clean and correct final output.
          
          find "$FINAL_RESULTS_DIR" -type f -name "*.txt" -print -exec sort -u -o {} {} \;

          echo "-> Final results have been merged and deduplicated without data loss."
          echo "Final results structure:"
          ls -R "$FINAL_RESULTS_DIR"
          
          echo "artifact_created=true" >> $GITHUB_OUTPUT

      - name: Upload Final Merged Artifact
        if: steps.consolidate.outputs.artifact_created == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: httpx-dsieve-merged-results
          path: final-merged-results/
          retention-days: 1

  create_commit:
    name: Create Commit in store-recon
    runs-on: ubuntu-latest
    needs: merge_results

    steps:
      - name: Download Merged Results Artifact
        uses: actions/download-artifact@v4
        with:
          name: httpx-dsieve-merged-results
          path: final-merged-results/

      - name: Checkout store-recon Repository
        uses: actions/checkout@v4
        with:
          repository: ${{ secrets.STORE_OWNER }}/${{ secrets.STORE }}
          token: ${{ secrets.PAT_FOR_SECONDRY }}
          path: store-recon

      - name: Synchronize and Push with Retry
        id: sync_and_push
        env:
          STORE_REPO_DIR: store-recon
          SOURCE_DIR: final-merged-results
          STORE_REPO_NAME: '${{ secrets.STORE_OWNER }}/${{ secrets.STORE }}'
             
          STORE_OWNER: ${{ secrets.STORE_OWNER}}
          STORE: ${{ secrets.STORE }}
          STORE_TOKEN: ${{ secrets.PAT_FOR_SECONDRY }}
          CORRELATION_ID: ${{ github.run_id }}

        run: |
          cd "${STORE}"

          echo "Configuring git user..."
          git config --global user.name "GitHub Actions Bot"
          git config --global user.email "github-actions-bot@users.noreply.github.com"

          # Retry loop to handle race conditions.
          for attempt in {1..5}
          do
            echo "[Attempt ${attempt}]"

            echo "-> Pulling latest changes from origin/main..."
            if ! git pull --rebase origin main; then
                echo "Pull failed. The remote history may have diverged in a complex way. Failing."
                # In a retry loop, it might be better to continue, but failing here is a safe default.
                exit 0
            fi
            
            echo "-> Running smart merge against the latest version..."
            MERGE_PERFORMED=false
            for domain_dir in "../$SOURCE_DIR"/*; do
              if [ -d "$domain_dir" ]; then
                domain_name=$(basename "$domain_dir")
                source_file="$domain_dir/httpx_result.txt"
                dest_file="results/$domain_name/httpx_result.txt"

                # --- START OF FIX ---
                # Only proceed if the source file actually exists.
                if [ ! -f "$source_file" ]; then
                    echo "  -> INFO: Source file '$source_file' not found. Skipping."
                    continue # Skip to the next domain directory
                fi
                
                # FIX: Sanitize the file to remove NULL characters that make Git treat it as binary.
                echo "  -> Sanitizing '$source_file'..."
                tr -d '\0' < "$source_file" > "$source_file.tmp" && mv "$source_file.tmp" "$source_file"
                # --- END OF FIX ---
                
                mkdir -p "$(dirname "$dest_file")"
                # Ensure the destination file exists before awk tries to read it.
                touch "$dest_file"
                
                echo "  -> Merging '$source_file' into '$dest_file'..."
                # Your smart merge command remains the same, but now operates on a clean file.
                awk 'FNR==NR {data[$1]=$0; next} {data[$1]=$0} END {for (key in data) print data[key]}' "$dest_file" "$source_file" | sort -u > "${dest_file}.tmp"
                mv "${dest_file}.tmp" "$dest_file"
                MERGE_PERFORMED=true
              fi
            done

            if [ "$MERGE_PERFORMED" = false ]; then
                echo "No source files found in artifact to process. Nothing to do."
                exit 0
            fi

            # Check for changes (no changes to this logic).
            if git diff --quiet --exit-code; then
              echo "✅ No new unique data to commit. Repository is already up-to-date."
              exit 0
            fi
            
            echo "-> Staging and committing changes..."
            git add results/
            git commit -m "feat(http-multiple): Automated http-multiple results from Correlation ID: ${CORRELATION_ID}"
            
            echo "-> Attempting to push..."
            if git push -v origin main; then
              echo "✅ Push successful on attempt ${attempt}!"
              exit 0 # Success!
            else
              echo "-> Push failed on attempt ${attempt}. Another workflow likely pushed ahead of us."
              # Reset local state to match remote before retrying.
              git reset --hard origin/main
              sleep $((RANDOM % 5 + 2))
            fi
          done

          echo "❌ Could not push changes after multiple attempts. Failing workflow."
          exit 0


  trigger_nuc_vuln:
    name: Trigger Nuc Vuln
    needs: [create_commit]
    
    if: success() 
    runs-on: ubuntu-latest
    steps:
      - name: Trigger next workflow
        uses: benc-uk/workflow-dispatch@v1
        with:
          workflow: nuc_scan.yml
          repo: Pcoder7/nuc-vuln
          token: ${{ secrets.PAT_TOKEN }}
          ref: main
          
          
          
          
